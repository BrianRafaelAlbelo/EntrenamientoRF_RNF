import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Paso 1: Cargar el conjunto de datos
dataset = load_dataset("MariaIsabel/FR_NFR_Spanish_requirements_classification")

# Paso 2: Preprocesamiento de datos
texts = dataset["train"]["REQUIREMENT"]
labels = dataset["train"]["FINAL_LABEL"]

# Dividir el conjunto de datos en entrenamiento y evaluación
train_texts, eval_texts, train_labels, eval_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

# Mapeo de etiquetas a valores numéricos
label_map = {label: idx for idx, label in enumerate(set(labels))}

# Convertir las etiquetas de texto a valores numéricos usando el mapeo
train_labels = [label_map[label] for label in train_labels]
eval_labels = [label_map[label] for label in eval_labels]

# Tokenizar los textos
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
eval_encodings = tokenizer(eval_texts, truncation=True, padding=True)

# Convertir las etiquetas a tensores de PyTorch
train_labels_tensor = torch.tensor(train_labels)
eval_labels_tensor = torch.tensor(eval_labels)

# Convertir las listas de input_ids y attention_mask a tensores
train_input_ids = torch.tensor(train_encodings["input_ids"])
train_attention_mask = torch.tensor(train_encodings["attention_mask"])
eval_input_ids = torch.tensor(eval_encodings["input_ids"])
eval_attention_mask = torch.tensor(eval_encodings["attention_mask"])

# Crear TensorDatasets con los tensores correspondientes
train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_mask, train_labels_tensor)
eval_dataset = torch.utils.data.TensorDataset(eval_input_ids, eval_attention_mask, eval_labels_tensor)

# Paso 3: Entrenamiento del modelo
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=len(label_map))
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Entrenamiento del modelo
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)
eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=8)

model.train()
for epoch in range(3):  # ajustar según necesidad
    print(f"Epoch {epoch+1}/{3}:")
    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))
    for step, batch in progress_bar:
        optimizer.zero_grad()
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        progress_bar.set_description(f"Training loss: {loss.item():.3f}")

# Guardar el modelo entrenado
torch.save(model.state_dict(), "distilbert_model2.pth")
print("Modelo guardado.")

# Paso 4: Evaluación del modelo
model.eval()
eval_loss = 0
eval_correct = 0
eval_total = 0

with torch.no_grad():
    for batch in tqdm(eval_loader, desc="Evaluating"):
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        eval_loss += outputs.loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        eval_correct += (predicted == labels).sum().item()
        eval_total += labels.size(0)

eval_accuracy = eval_correct / eval_total
print("Accuracy:", eval_accuracy)

# Paso 5: Inferencia
new_text = "el sistema debera ejecutar en 10 segundos"
inputs = tokenizer(new_text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)
predicted_label_id = torch.argmax(outputs.logits).item()
predicted_label = "Funcional" if predicted_label_id == 1 else "No funcional"
print("Nuevo texto:", new_text)
print("Predicción:", predicted_label)
